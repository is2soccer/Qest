import whisper
import torch
import time
import openai
import os
from dotenv import load_dotenv
from pydub import AudioSegment
from diarizer import save_diarized_transcript

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ” í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {'GPU' if device == 'cuda' else 'CPU'}")

# Whisper ëª¨ë¸ ë¡œë“œ
model = whisper.load_model("large-v3").to(device)

# .env íŒŒì¼ ë¡œë“œ ë° OpenAI API í‚¤ ì„¤ì •
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=api_key)

def transcribe_audio_local(audio_file, output_text_file):
    print("ğŸ§  Whisper ë¡œì»¬ ë³€í™˜ ì‹œì‘...")
    start_time = time.time()
    result = model.transcribe(audio_file, language="ko")
    end_time = time.time()
    processing_time = round(end_time - start_time, 2)
    with open(output_path, "w", encoding="utf-8") as f:
        for segment in result["segments"]:
            f.write(f"{segment['start']} {segment['end']} {segment['text']}\n")
    print(f"ğŸ“ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {output_text_file}")
    print(f"â³ Whisper ë¡œì»¬ ë³€í™˜ ì™„ë£Œ. ì‹¤í–‰ ì‹œê°„: {processing_time}ì´ˆ")
    print(f"ğŸš€ Whisper ì‹¤í–‰ ì¥ì¹˜: {'GPU' if device == 'cuda' else 'CPU'}")

    # ğŸ”¥ Pyannoteë¡œ í™”ì êµ¬ë¶„ ì ìš©
    diarized_transcript_path = save_diarized_transcript(audio_file, output_text_file)
    return diarized_transcript_path

def transcribe_audio_api(audio_file, output_text_file):
    print("â˜ï¸ OpenAI API ë³€í™˜ ì‹œì‘...")
    
    # ì§€ì›ë˜ëŠ” ì˜¤ë””ì˜¤ í™•ì¥ì í™•ì¸ ë° ë³€í™˜
    file_extension = os.path.splitext(audio_file)[1].lower()
    if file_extension == ".wav":
        mp3_file = audio_file.replace(".wav", ".mp3")
    elif file_extension == ".m4a":
        mp3_file = audio_file.replace(".m4a", ".mp3")
    else:
        print(f"âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
        return
    
    try:
        audio = AudioSegment.from_file(audio_file, format=file_extension[1:])  # í™•ì¥ìì—ì„œ '.' ì œê±° í›„ ì‚¬ìš©
        audio.export(mp3_file, format="mp3", bitrate="64k")  # 64kbpsë¡œ ì••ì¶•í•˜ì—¬ í¬ê¸° ì¤„ì´ê¸°
        print(f"ğŸ”„ {file_extension.upper()} â†’ MP3 ë³€í™˜ ì™„ë£Œ: {mp3_file}")
    except Exception as e:
        print(f"âŒ MP3 ë³€í™˜ ì˜¤ë¥˜: {e}")
        return
    
 # ğŸ¤ OpenAI Whisper API ë³€í™˜ ì‹¤í–‰
    try:
        with open(mp3_file, "rb") as audio:
            response = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio,
                language="ko",
                response_format="verbose_json"  # JSON í˜•íƒœë¡œ ì‘ë‹µ ë°›ê¸°
            )

        # âœ… API ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³  ë³€í™˜
        segments = response.segments if isinstance(response.segments, list) else list(response.segments)

        # ğŸ“ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ì €ì¥
        with open(output_text_file, "w", encoding="utf-8") as f:
            for segment in segments:
                f.write(f"{segment.start} {segment.end} {segment.text}\n")

        print(f"âœ… OpenAI API ë³€í™˜ ì™„ë£Œ: {output_text_file}")

    except Exception as e:
        print(f"âŒ OpenAI API ë³€í™˜ ì˜¤ë¥˜: {e}")
        return

    # ğŸ—‘ï¸ MP3 íŒŒì¼ ì‚­ì œ (ì„ì‹œ ë³€í™˜ íŒŒì¼ì´ë¯€ë¡œ ì‚­ì œ)
    os.remove(mp3_file)
    print(f"ğŸ—‘ï¸ ì„ì‹œ MP3 íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {mp3_file}")

    # ğŸ”¥ Pyannote í™”ì êµ¬ë¶„ ì‹¤í–‰
    diarized_transcript_path = save_diarized_transcript(audio_file, output_text_file)

    return diarized_transcript_path


    
