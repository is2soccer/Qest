from pyannote.audio.pipelines.speaker_diarization import SpeakerDiarization
from pyannote.core import Segment
import os
from dotenv import load_dotenv

# ğŸ”¹ .env íŒŒì¼ì—ì„œ Hugging Face Access Token ë¡œë“œ
load_dotenv()
HF_TOKEN = os.getenv("HUGGINGFACE_ACCESS_TOKEN")

# ğŸ”¥ Pyannote ëª¨ë¸ ë¡œë“œ ì‹œ ì¸ì¦ í† í° ì‚¬ìš©
pipeline = SpeakerDiarization.from_pretrained(
    "pyannote/speaker-diarization", 
    use_auth_token=HF_TOKEN
)

def diarize_audio(audio_path):
    """
    Pyannoteë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ í™”ìë¥¼ êµ¬ë¶„í•˜ê³  íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("ğŸ” í™”ì ë¶„ë¦¬ ì‹¤í–‰ ì¤‘...")
    diarization = pipeline(audio_path)

    speaker_segments = []
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        speaker_segments.append({
            "start": turn.start,
            "end": turn.end,
            "speaker": speaker
        })

    return speaker_segments

def find_best_matching_speaker(start_time, end_time, diarized_segments):
    """
    Whisper í…ìŠ¤íŠ¸ì˜ íƒ€ì„ìŠ¤íƒ¬í”„(start_time, end_time)ì™€ ê°€ì¥ ì ì ˆí•œ Pyannote í™”ìë¥¼ ë§¤ì¹­
    """
    best_speaker = "Unknown"
    min_diff = float("inf")  # ìµœì†Œ ì‹œê°„ ì°¨ì´ ì´ˆê¸°í™”

    for segment in diarized_segments:
        # Whisperì˜ í…ìŠ¤íŠ¸ê°€ Pyannote êµ¬ê°„ ë‚´ì— ìˆì„ ê²½ìš°, í•´ë‹¹ í™”ì ì„ íƒ
        if segment["start"] <= start_time <= segment["end"] or segment["start"] <= end_time <= segment["end"]:
            return segment["speaker"]

        # Whisper íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ê°€ì¥ ê°€ê¹Œìš´ Pyannote êµ¬ê°„ì„ ì°¾ê¸°
        start_diff = abs(segment["start"] - start_time)
        end_diff = abs(segment["end"] - end_time)
        avg_diff = (start_diff + end_diff) / 2  # í‰ê·  ê±°ë¦¬ ê³„ì‚°

        if avg_diff < min_diff:
            min_diff = avg_diff
            best_speaker = segment["speaker"]

    return best_speaker

def save_diarized_transcript(audio_path, transcript_path):
    """
    Whisper ë³€í™˜ëœ í…ìŠ¤íŠ¸ì™€ Pyannoteì˜ í™”ì êµ¬ë¶„ ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    """
    diarized_segments = diarize_audio(audio_path)

    # ğŸ” Whisper ë³€í™˜ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
    if not os.path.exists(transcript_path):
        print(f"âŒ ì˜¤ë¥˜: ë³€í™˜ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ ({transcript_path})")
        return

    with open(transcript_path, "r", encoding="utf-8") as f:
        transcript_lines = f.readlines()

    if not transcript_lines:
        print(f"âŒ ì˜¤ë¥˜: ë³€í™˜ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ë¹„ì–´ ìˆìŒ ({transcript_path})")
        return

    combined_results = []
    for line in transcript_lines:
        line = line.strip()
        if not line:
            continue

        parts = line.split(" ", 2)
        if len(parts) < 3:
            continue

        try:
            start_time, end_time, text = float(parts[0]), float(parts[1]), parts[2]
        except ValueError:
            print(f"âš ï¸ ê²½ê³ : ì˜ëª»ëœ í˜•ì‹ì˜ ì¤„ ë°œê²¬ - {line}")
            continue

        # ğŸ” ê°€ì¥ ì ì ˆí•œ í™”ì ì°¾ê¸° (Pyannote êµ¬ê°„ ë‚´ ë˜ëŠ” ê°€ì¥ ê°€ê¹Œìš´ í™”ì ì„ íƒ)
        speaker = find_best_matching_speaker(start_time, end_time, diarized_segments)

        combined_results.append({
            "start": start_time,
            "end": end_time,
            "speaker": speaker,
            "text": text
        })

    # âœ… ê²°ê³¼ ì €ì¥
    diarized_output_path = transcript_path.replace(".txt", "_diarized.txt")
    with open(diarized_output_path, "w", encoding="utf-8") as f:
        for item in combined_results:
            f.write(f"[{item['start']:.2f} - {item['end']:.2f}] Speaker {item['speaker']}: {item['text']}\n")

    print(f"âœ… í™”ì êµ¬ë¶„ ì™„ë£Œ! ê²°ê³¼ ì €ì¥: {diarized_output_path}")
    return diarized_output_path
